//! Metrics Aggregation Service
//!
//! Consumes events from Kafka, aggregates metrics, and writes to TimescaleDB.
//! Features:
//! - Kafka consumer with consumer group
//! - Time-window aggregations (1m, 5m, 15m, 1h)
//! - TimescaleDB batch writes
//! - Redis caching for intermediate state
//! - Prometheus metrics
//! - Graceful shutdown with offset commit

use chrono::{DateTime, Duration as ChronoDuration, Utc};
use dashmap::DashMap;
use llm_analytics_hub::{AggregatedMetric, AnalyticsEvent, StatisticalMeasures, TimeWindow};
use prometheus::{
    register_counter_vec, register_histogram_vec, register_int_gauge, CounterVec, Encoder,
    HistogramVec, IntGauge, TextEncoder,
};
use rdkafka::consumer::{CommitMode, Consumer, StreamConsumer};
use rdkafka::{ClientConfig, Message};
use redis::aio::ConnectionManager;
use serde::{Deserialize, Serialize};
use sqlx::postgres::PgPoolOptions;
use sqlx::{PgPool, Postgres};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::signal;
use tokio::sync::RwLock;
use tokio::time::{interval, Duration};
use tracing::{error, info, warn};

/// Application state
struct AppState {
    db_pool: PgPool,
    redis: ConnectionManager,
    metrics: Arc<Metrics>,
    aggregator: Arc<MetricsAggregator>,
}

/// Prometheus metrics
struct Metrics {
    events_consumed: CounterVec,
    metrics_aggregated: CounterVec,
    db_writes: CounterVec,
    db_write_duration: HistogramVec,
    kafka_lag: IntGauge,
    aggregation_duration: HistogramVec,
}

impl Metrics {
    fn new() -> anyhow::Result<Self> {
        Ok(Self {
            events_consumed: register_counter_vec!(
                "llm_metrics_events_consumed_total",
                "Total events consumed from Kafka",
                &["topic", "partition"]
            )?,
            metrics_aggregated: register_counter_vec!(
                "llm_metrics_aggregated_total",
                "Total metrics aggregated",
                &["metric_type", "window"]
            )?,
            db_writes: register_counter_vec!(
                "llm_db_writes_total",
                "Total database writes",
                &["table", "status"]
            )?,
            db_write_duration: register_histogram_vec!(
                "llm_db_write_duration_seconds",
                "Database write duration",
                &["table"]
            )?,
            kafka_lag: register_int_gauge!("llm_kafka_consumer_lag", "Kafka consumer lag")?,
            aggregation_duration: register_histogram_vec!(
                "llm_aggregation_duration_seconds",
                "Metrics aggregation duration",
                &["window"]
            )?,
        })
    }
}

/// Configuration
#[derive(Debug, Clone)]
struct Config {
    kafka_brokers: String,
    kafka_topic: String,
    kafka_group_id: String,
    database_url: String,
    redis_url: String,
    aggregation_interval_secs: u64,
}

impl Config {
    fn from_env() -> Self {
        Self {
            kafka_brokers: std::env::var("KAFKA_BROKERS")
                .unwrap_or_else(|_| "kafka.llm-analytics.svc.cluster.local:9092".to_string()),
            kafka_topic: std::env::var("KAFKA_TOPIC").unwrap_or_else(|_| "llm-events".to_string()),
            kafka_group_id: std::env::var("KAFKA_GROUP_ID")
                .unwrap_or_else(|_| "metrics-aggregation".to_string()),
            database_url: std::env::var("DATABASE_URL").unwrap_or_else(|_| {
                "postgres://admin:password@timescaledb.llm-analytics.svc.cluster.local:5432/llm_analytics".to_string()
            }),
            redis_url: std::env::var("REDIS_URL")
                .unwrap_or_else(|_| "redis://redis.llm-analytics.svc.cluster.local:6379".to_string()),
            aggregation_interval_secs: std::env::var("AGGREGATION_INTERVAL_SECS")
                .unwrap_or_else(|_| "60".to_string())
                .parse()
                .expect("Invalid AGGREGATION_INTERVAL_SECS"),
        }
    }
}

/// Metrics aggregator
struct MetricsAggregator {
    windows: Arc<DashMap<String, WindowAggregation>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
struct WindowAggregation {
    window_start: DateTime<Utc>,
    window_end: DateTime<Utc>,
    values: Vec<f64>,
    count: u64,
    sum: f64,
    min: f64,
    max: f64,
}

impl WindowAggregation {
    fn new(window_start: DateTime<Utc>, window_end: DateTime<Utc>) -> Self {
        Self {
            window_start,
            window_end,
            values: Vec::new(),
            count: 0,
            sum: 0.0,
            min: f64::MAX,
            max: f64::MIN,
        }
    }

    fn add_value(&mut self, value: f64) {
        self.values.push(value);
        self.count += 1;
        self.sum += value;
        self.min = self.min.min(value);
        self.max = self.max.max(value);
    }

    fn calculate_statistics(&self) -> StatisticalMeasures {
        let mean = if self.count > 0 {
            self.sum / self.count as f64
        } else {
            0.0
        };

        let mut sorted_values = self.values.clone();
        sorted_values.sort_by(|a, b| a.partial_cmp(b).unwrap());

        let median = if sorted_values.is_empty() {
            0.0
        } else if sorted_values.len() % 2 == 0 {
            let mid = sorted_values.len() / 2;
            (sorted_values[mid - 1] + sorted_values[mid]) / 2.0
        } else {
            sorted_values[sorted_values.len() / 2]
        };

        let p95_idx = ((sorted_values.len() as f64 * 0.95).ceil() as usize).min(sorted_values.len()) - 1;
        let p99_idx = ((sorted_values.len() as f64 * 0.99).ceil() as usize).min(sorted_values.len()) - 1;

        let p95 = if sorted_values.is_empty() {
            0.0
        } else {
            sorted_values[p95_idx]
        };

        let p99 = if sorted_values.is_empty() {
            0.0
        } else {
            sorted_values[p99_idx]
        };

        // Calculate standard deviation
        let variance = if self.count > 0 {
            let mean_diff_sq: f64 = self.values.iter().map(|v| (v - mean).powi(2)).sum();
            mean_diff_sq / self.count as f64
        } else {
            0.0
        };
        let std_dev = variance.sqrt();

        StatisticalMeasures {
            avg: mean,
            min: self.min,
            max: self.max,
            p50: median,
            p95,
            p99,
            stddev: Some(std_dev),
            count: self.count,
            sum: self.sum,
        }
    }
}

impl MetricsAggregator {
    fn new() -> Self {
        Self {
            windows: Arc::new(DashMap::new()),
        }
    }

    fn aggregate_event(&self, event: &AnalyticsEvent) {
        let window_key = format!("{:?}_{}", event.common.event_type, event.common.timestamp.timestamp() / 60);

        // For simplicity, we're just counting events
        // In production, extract actual metric values from event payload
        let value = 1.0;

        self.windows
            .entry(window_key.clone())
            .or_insert_with(|| {
                let window_start = event.common.timestamp;
                let window_end = window_start + ChronoDuration::minutes(1);
                WindowAggregation::new(window_start, window_end)
            })
            .add_value(value);
    }

    async fn flush_to_db(&self, pool: &PgPool, metrics: &Arc<Metrics>) -> anyhow::Result<()> {
        let windows: Vec<_> = self.windows.iter().map(|e| (e.key().clone(), e.value().clone())).collect();

        for (key, window) in windows {
            let stats = window.calculate_statistics();
            let timer = metrics.db_write_duration.with_label_values(&["metrics"]).start_timer();

            // Insert aggregated metrics into TimescaleDB
            // Using sqlx::query instead of sqlx::query! to avoid DATABASE_URL requirement at compile time
            let result = sqlx::query(
                r#"
                INSERT INTO aggregated_metrics (
                    window_start, window_end, metric_name, metric_type,
                    count, sum, mean, median, std_dev, min, max, p50, p95, p99
                )
                VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14)
                ON CONFLICT (window_start, metric_name) DO UPDATE
                SET count = EXCLUDED.count,
                    sum = EXCLUDED.sum,
                    mean = EXCLUDED.mean,
                    median = EXCLUDED.median,
                    std_dev = EXCLUDED.std_dev,
                    min = EXCLUDED.min,
                    max = EXCLUDED.max,
                    p50 = EXCLUDED.p50,
                    p95 = EXCLUDED.p95,
                    p99 = EXCLUDED.p99
                "#,
            )
            .bind(&window.window_start)
            .bind(&window.window_end)
            .bind(&key)
            .bind("counter")
            .bind(window.count as i64)
            .bind(window.sum)
            .bind(stats.avg)
            .bind(stats.p50)
            .bind(stats.stddev)
            .bind(stats.min)
            .bind(stats.max)
            .bind(stats.p50)
            .bind(stats.p95)
            .bind(stats.p99)
            .execute(pool)
            .await;

            timer.observe_duration();

            match result {
                Ok(_) => {
                    metrics.db_writes.with_label_values(&["metrics", "success"]).inc();
                    self.windows.remove(&key);
                }
                Err(e) => {
                    error!("Failed to write metrics to database: {}", e);
                    metrics.db_writes.with_label_values(&["metrics", "error"]).inc();
                }
            }
        }

        Ok(())
    }
}

#[tokio::main]
async fn main() -> anyhow::Result<()> {
    // Initialize tracing
    tracing_subscriber::fmt()
        .with_env_filter(
            tracing_subscriber::EnvFilter::try_from_default_env()
                .unwrap_or_else(|_| "metrics_aggregation=info".into()),
        )
        .init();

    info!("Starting Metrics Aggregation Service v{}", env!("CARGO_PKG_VERSION"));

    // Load configuration
    let config = Config::from_env();
    info!(?config, "Configuration loaded");

    // Initialize metrics
    let metrics = Arc::new(Metrics::new()?);

    // Initialize database pool
    let db_pool = PgPoolOptions::new()
        .max_connections(10)
        .connect(&config.database_url)
        .await?;

    info!("Database connection pool initialized");

    // Initialize Redis client
    let redis_client = redis::Client::open(config.redis_url.as_str())?;
    let redis_conn = ConnectionManager::new(redis_client).await?;

    info!("Redis connection established");

    // Create aggregator
    let aggregator = Arc::new(MetricsAggregator::new());

    // Create Kafka consumer
    let consumer: StreamConsumer = ClientConfig::new()
        .set("bootstrap.servers", &config.kafka_brokers)
        .set("group.id", &config.kafka_group_id)
        .set("enable.auto.commit", "false")
        .set("auto.offset.reset", "earliest")
        .set("client.id", "metrics-aggregation-service")
        .create()?;

    consumer.subscribe(&[&config.kafka_topic])?;
    info!("Subscribed to Kafka topic: {}", config.kafka_topic);

    // Spawn aggregation flush task
    let flush_pool = db_pool.clone();
    let flush_aggregator = aggregator.clone();
    let flush_metrics = metrics.clone();
    let flush_interval = config.aggregation_interval_secs;
    tokio::spawn(async move {
        let mut interval = interval(Duration::from_secs(flush_interval));
        loop {
            interval.tick().await;
            if let Err(e) = flush_aggregator.flush_to_db(&flush_pool, &flush_metrics).await {
                error!("Failed to flush metrics: {}", e);
            }
        }
    });

    // Main consumption loop
    let mut shutdown = false;
    while !shutdown {
        tokio::select! {
            message_result = consumer.recv() => {
                match message_result {
                    Ok(m) => {
                        if let Some(payload) = m.payload() {
                            match serde_json::from_slice::<AnalyticsEvent>(payload) {
                                Ok(event) => {
                                    let partition = m.partition().to_string();
                                    metrics.events_consumed
                                        .with_label_values(&[&config.kafka_topic, &partition])
                                        .inc();

                                    aggregator.aggregate_event(&event);

                                    // Commit offset
                                    if let Err(e) = consumer.commit_message(&m, CommitMode::Async) {
                                        warn!("Failed to commit offset: {}", e);
                                    }
                                }
                                Err(e) => {
                                    error!("Failed to deserialize event: {}", e);
                                }
                            }
                        }
                    }
                    Err(e) => {
                        error!("Kafka consumer error: {}", e);
                    }
                }
            }
            _ = signal::ctrl_c() => {
                info!("Received shutdown signal");
                shutdown = true;
            }
        }
    }

    // Final flush before shutdown
    info!("Performing final metrics flush");
    aggregator.flush_to_db(&db_pool, &metrics).await?;

    info!("Service shutdown complete");
    Ok(())
}
